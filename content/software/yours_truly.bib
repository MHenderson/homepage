
@book{henderson_collaborative_nodate,
	title = {Collaborative {Development} of {System} {Architecture}- a {Tool} for {Coping} with {Inconsistency}},
	abstract = {Very large systems have an architecture that is designed to allow them to evolve through a long life. Such systems are developed by teams of architects. One of the first things the architects do is make a model of their architecture. This model constitutes the formal architecture description based on which software engineers will eventually build the real system. The architecture model is normally governed by a specialised metamodel whose rules determine the consistency and completeness of the description. The development of a system architecture is carried out cooperatively but independently by team members. Consequently it is quite normal for the architecture description as a whole to be both incomplete and inconsistent. The architects strive to eventually produce a complete overall (i.e. merged) description and to eliminate the inconsistencies. By means of an example, we show how and why the architecture model and the metamodel must co-evolve. We describe a design tool that we have developed to support this process of co-evolution. The tool allows a team of architects to detect inconsistencies in their separate and merged models. The tool tolerates inconsistencies. It produces reports of inconsistencies which then become targets for removal as the whole architecture description evolves. 1.},
	author = {Henderson, Peter and Henderson, Matthew J.}
}

@inproceedings{henderson_consistency_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Consistency {Checking} for {Component} {Reuse} in {Open} {Systems}},
	isbn = {978-3-642-04210-2 978-3-642-04211-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04211-9_1},
	doi = {10.1007/978-3-642-04211-9_1},
	abstract = {Large scale Open Systems are built from reusable components in such a way that enhanced system functionality can be deployed, quickly and effectively, simply by plugging in a few new or revised components. At the architectural level, when new variations of a system are being planned by (re)configuring reusable components, the architecture description can itself become very large and complex. Consequently, the opportunities for inconsistency abound. This paper describes a method of architecture description that allows a significant amount of consistency checking to be done throughout the process of developing a system architecture description. An architectural design tool is described that supports consistency checking. This tool is designed to support component reuse, incremental development and collaborative working, essential for developing the architecture description of large systems.},
	language = {en},
	booktitle = {Formal {Foundations} of {Reuse} and {Domain} {Engineering}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Henderson, Peter and Henderson, Matthew J.},
	month = sep,
	year = {2009},
	pages = {1--10}
}

@article{davis_modeling_2010,
	title = {Modeling {Sudoku} {Puzzles} with {Python}},
	abstract = {The popular Sudoku puzzles which appear daily in newspapers the world over have, lately, attracted the attention of mathematicians and computer scientists. There are many, difﬁcult, unsolved problems about Sudoku puzzles and their generalizations which make them especially interesting to mathematicians. Also, as is well-known, the generalization of the Sudoku puzzle to larger dimension is an NP-complete problem and therefore of substantial interest to computer scientists.},
	language = {en},
	author = {Davis, Sean and Henderson, Matthew and Smith, Andrew},
	year = {2010},
	pages = {7}
}

@article{henderson_completing_2004,
	title = {Completing an edge-colouring of {K} 2m with {K} r and independent edges precoloured},
	volume = {70},
	doi = {10.1112/S0024610704005526},
	abstract = {Those (2m - 1)-edge-colourings of a spanning subgraph of K2m, consisting of Kr and independent edges, that can be embedded in a (2m - l)-edge-colouring of K2m are characterised.},
	journal = {Journal of the London Mathematical Society. Second Series},
	author = {Henderson, M. J. and Hilton, Anthony},
	month = dec,
	year = {2004}
}

@inproceedings{henderson_mixture_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mixture of {Vector} {Experts}},
	isbn = {978-3-540-29242-5 978-3-540-31696-1},
	url = {https://link.springer.com/chapter/10.1007/11564089_30},
	doi = {10.1007/11564089_30},
	abstract = {We describe and analyze an algorithm for predicting a sequence of n-dimensional binary vectors based on a set of experts making vector predictions in [0,1] n . We measure the loss of individual predictions by the 2-norm between the actual outcome vector and the prediction. The loss of an expert is then the sum of the losses experienced on individual trials. We obtain bounds for the loss of our expert algorithm in terms of the loss of the best expert analogous to the well-known results for scalar experts making real-valued predictions of a binary outcome.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Henderson, Matthew and Shawe-Taylor, John and Žerovnik, Janez},
	month = oct,
	year = {2005},
	pages = {386--398}
}